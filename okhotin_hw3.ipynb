{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №3 - Дерево решений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** до 30 апреля 2018, 06:00   \n",
    "**Штраф за опоздание:** -2 балла после 06:00 30 апреля, -4 балла после 06:00 7 мая, -6 баллов после 06:00 14 мая, -8 баллов после 06:00 21 мая\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла   \n",
    "\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий в slack @alkhamush\n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Задание 1 (3 балла)\n",
    "Разберитесь в коде MyDecisionTreeClassifier, который уже частично реализован. Допишите код там, где написано \"Ваш код\". Ваша реализация дерева должна работать по точности не хуже DecisionTreeClassifier из sklearn. Точность проверяется на wine и Speed Dating Data.\n",
    "\n",
    "###### Задание 2 (3 балла)\n",
    "Добиться скорости работы на fit не медленнее чем в 10 раз sklearn на данных wine и Speed Dating Data. \n",
    "Для этого используем numpy.\n",
    "\n",
    "###### Задание 3 (2 балла)\n",
    "Добавьте функционал, который определяет значения feature importance. Выведите 10 главных фичей под пунктом Задание 4 (уже написано ниже) для MyDecisionTreeClassifier и DecisionTreeClassifier так, чтобы сразу были видны выводы и по MyDecisionTreeClassifier, и по DecisionTreeClassifier. Используем данные Speed Dating Data.\n",
    "\n",
    "###### Задание 4 (2 балла)\n",
    "С помощью GridSearchCV или RandomSearchCV подберите наиболее оптимальные параметры для случайного леса (Выберете 2-3 параметра). Используем данные Speed Dating Data. Задание реализуйте под пунктом Задание 5 (уже написано ниже)\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw3.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст. В противном случае -1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#%matplotlib inline\n",
    "#%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    NON_LEAF_TYPE = 0\n",
    "    LEAF_TYPE = 1\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=990, sufficient_share=1.0, criterion='gini', max_features=None):\n",
    "        self.purity_index = 1\n",
    "        self.tree = dict()\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.sufficient_share = sufficient_share\n",
    "        self.num_class = -1\n",
    "        self.feature_importances_ = None\n",
    "        if criterion == 'gini':\n",
    "            self.G_function = self.__gini\n",
    "        elif criterion == 'entropy':\n",
    "            self.G_function = self.__entropy\n",
    "        elif criterion == 'misclass':\n",
    "            self.G_function = self.__misclass\n",
    "        else:\n",
    "            print('invalid criterion name')\n",
    "            raise\n",
    "\n",
    "        if max_features == 'sqrt':\n",
    "            self.get_feature_ids = self.__get_feature_ids_sqrt\n",
    "        elif max_features == 'log2':\n",
    "            self.get_feature_ids = self.__get_feature_ids_log2\n",
    "        elif max_features == None:\n",
    "            self.get_feature_ids = self.__get_feature_ids_N\n",
    "        else:\n",
    "            print('invalid max_features name')\n",
    "            raise\n",
    "\n",
    "    def __gini(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = l_s.astype('float')\n",
    "        r_s = r_s.astype('float')\n",
    "        return l_s + r_s - np.sum(l_c**2, axis=1) / l_s - np.sum(r_c**2, axis=1) / r_s\n",
    "    \n",
    "    def __entropy(self, l_c, l_s, r_c, r_s):\n",
    "        l_entropy = np.sum(l_c * np.log(l_c), axis=1) - l_s * np.log(l_s)\n",
    "        r_entropy = np.sum(r_c * np.log(r_c), axis=1) - r_s * np.log(r_s)\n",
    "        return (l_entropy + r_entropy) / 2.0\n",
    "        \n",
    "    def __misclass(self, l_c, l_s, r_c, r_s):\n",
    "        l_misclass = 1 - l_c / np.repeat(l_s, l_c.shape[1]).reshape(-1, l_c.shape[1]) \n",
    "        r_misclass = 1 - r_c / np.repeat(r_s, r_c.shape[1]).reshape(-1, r_c.shape[1])\n",
    "        return (l_misclass + r_misclass) / 2.0\n",
    "\n",
    "    def __get_feature_ids_sqrt(self, n_feature):\n",
    "        feature_ids = np.arange(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:int(np.sqrt(n_feature))]\n",
    "        \n",
    "    def __get_feature_ids_log2(self, n_feature):\n",
    "        feature_ids = range(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:int(np.log(n_feature))]\n",
    "\n",
    "    def __get_feature_ids_N(self, n_feature):\n",
    "        return range(n_feature)\n",
    "    \n",
    "    def __sort_samples(self, x, y):\n",
    "        sorted_idx = x.argsort()\n",
    "        return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "    def __div_samples(self, x, y, feature_id, threshold):\n",
    "        left_mask = x[:, int(feature_id)] > threshold\n",
    "        right_mask = ~left_mask\n",
    "        return x[left_mask], x[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def __find_threshold(self, x, y):\n",
    "        x_data, y_data = self.__sort_samples(x, y)\n",
    "        split_y = y_data[self.min_samples_split:-self.min_samples_split]\n",
    "        critical_points_ids = np.where(split_y[1:] != split_y[:-1])[0] + self.min_samples_split + 1\n",
    "        \n",
    "        if len(critical_points_ids) == 0:\n",
    "            return np.inf, None\n",
    "        \n",
    "        points_delta = critical_points_ids - np.append([self.min_samples_split], critical_points_ids[:-1])\n",
    "        mas = np.zeros((critical_points_ids.shape[0], self.num_class))\n",
    "        mas[np.arange(critical_points_ids.shape[0]), y_data[critical_points_ids - 1]] = 1\n",
    "        cls = mas * points_delta.reshape(-1, 1)\n",
    "        cls[0] = cls[0] + np.bincount(y_data[:self.min_samples_split], minlength=self.num_class)\n",
    "        \n",
    "        tmp = np.bincount(y_data)\n",
    "        template = np.hstack((tmp, np.zeros(self.num_class - tmp.shape[0])))  \n",
    "        l_c = np.cumsum(cls, axis=0)  \n",
    "        r_c = template - l_c\n",
    "        l_s = critical_points_ids.reshape(l_c.shape[0])\n",
    "        r_s = y_data.shape[0] - l_s\n",
    "\n",
    "        g = self.G_function(l_c, l_s, r_c, r_s)\n",
    "        id_x = np.argmin(g)\n",
    "    \n",
    "        elem_id = l_s[id_x]\n",
    "        return g[id_x], (x_data[elem_id-1] + x_data[elem_id]) / 2.0\n",
    "            \n",
    "\n",
    "    def __fit_node(self, x, y, node_id, depth):\n",
    "        \n",
    "        if ((self.max_depth <= depth) or \\\n",
    "            (self.sufficient_share <= sum((np.unique(y, return_counts=True)[1] / y.shape[0])**2)) or \\\n",
    "            (y.shape[0] < self.min_samples_split)):\n",
    "            \n",
    "            self.tree[node_id] = [self.__class__.LEAF_TYPE, np.argmax(np.bincount(y))]\n",
    "            \n",
    "        else:\n",
    "            features = self.get_feature_ids(x.shape[1])\n",
    "            mas = np.array([self.__find_threshold(x[:, f], y) for f in features])\n",
    "            best_feature = np.argmin(mas[:,0])\n",
    "            best_threshold = mas[best_feature, 1]\n",
    "            \n",
    "            if best_threshold == None:\n",
    "                self.purity_index *= sum((np.unique(y, return_counts=True)[1] / y.shape[0])**2)\n",
    "                self.tree[node_id] = [self.__class__.LEAF_TYPE, np.argmax(np.bincount(y))] \n",
    "            else:\n",
    "                l_x, r_x, l_y, r_y = self.__div_samples(x, y, best_feature, best_threshold)\n",
    "                if (l_x.size == 0) or (r_x.size == 0):\n",
    "                    self.tree[node_id] = [self.__class__.LEAF_TYPE, np.argmax(np.bincount(y))]\n",
    "                else:   \n",
    "                    self.tree[node_id] = [self.__class__.NON_LEAF_TYPE, best_feature, best_threshold]\n",
    "                    self.__fit_node(l_x, l_y, 2*node_id + 1, depth+1)\n",
    "                    self.__fit_node(r_x, r_y, 2*node_id + 2, depth+1)\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.num_class = np.unique(y).shape[0]\n",
    "        self.dim_X = x.shape[1]\n",
    "        self.__fit_node(x, y, 0, 0) \n",
    "\n",
    "    def __predict_class(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_class(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_class(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[1]\n",
    "\n",
    "    def __predict_probs(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_probs(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_probs(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[2]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.array([self.__predict_class(x, 0) for x in X])\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        return np.array([self.__predict_probs(x, 0) for x in X])\n",
    "\n",
    "    def fit_predict(self, x_train, y_train, predicted_x):\n",
    "        self.fit(x_train, y_train)\n",
    "        return self.predict(predicted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка качества работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8857142857142858"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8857142857142858"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных Speed Dating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Speed Dating Data.csv', encoding='cp1251') \n",
    "\n",
    "for i in df.columns:\n",
    "    if type(df[i][0]) not in [np.float64, np.int64, float]:\n",
    "        df.drop(i,axis=1, inplace=True)\n",
    "\n",
    "for i in df.columns:\n",
    "    if df[i].isnull().sum()>500:\n",
    "        df.drop(i,axis=1, inplace=True)\n",
    "\n",
    "df = df.fillna(-10000)\n",
    "df.drop('id', axis=1)\n",
    "y = df.loc[:, 'gender'].values\n",
    "X = df.drop('gender', axis=1).iloc[:,:].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка скорости работы на Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 121 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на Speed Dating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9880662139992709"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9695690948802982"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_feature_importances(my_clf):\n",
    "    \n",
    "    def recur_calc_feature_importances(node_id, feature_importances, count): \n",
    "        node = my_clf.tree[node_id]\n",
    "        if node[0] == my_clf.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, _ = node\n",
    "            feature_importances[feature_id] += 1\n",
    "            count += 1\n",
    "            \n",
    "            _, tmp = recur_calc_feature_importances(2 * node_id + 1, feature_importances, 0)\n",
    "            count += tmp\n",
    "            _, tmp = recur_calc_feature_importances(2 * node_id + 2, feature_importances, 0)\n",
    "            count += tmp\n",
    "        \n",
    "        return feature_importances, count\n",
    "    \n",
    "    feature_importances = np.zeros(my_clf.dim_X)\n",
    "    feature_importances, count = recur_calc_feature_importances(0, feature_importances, 0)\n",
    "    feature_importances /= count\n",
    "    \n",
    "    return feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My:                   Skearn:\n",
      "id  importance        id  importance\n",
      " 0  9.401709e-02      56  2.741704e-01\n",
      "56  5.982906e-02      62  2.073823e-01\n",
      " 1  4.273504e-02      15  1.037725e-01\n",
      "35  4.273504e-02       0  3.920737e-02\n",
      "60  4.273504e-02      53  2.503208e-02\n",
      "66  3.418803e-02       9  2.466296e-02\n",
      " 9  3.418803e-02      60  2.372600e-02\n",
      "62  3.418803e-02      33  2.189670e-02\n",
      "17  3.418803e-02      68  1.950069e-02\n",
      "49  2.564103e-02      48  1.903870e-02\n"
     ]
    }
   ],
   "source": [
    "my_feature_list = calc_feature_importances(my_clf)\n",
    "my_idx_list = my_feature_list.argsort()[-10:][::-1]\n",
    "my_res = [[idx, my_feature_list[idx]] for idx in my_idx_list]\n",
    "\n",
    "feature_list = clf.feature_importances_\n",
    "idx_list = feature_list.argsort()[-10:][::-1]\n",
    "res = [[idx, feature_list[idx]] for idx in idx_list]\n",
    "\n",
    "print(\"My:                   Skearn:\")\n",
    "print(\"id  importance        id  importance\")\n",
    "for i in range(len(res)):\n",
    "    print('{:2}'.format(my_res[i][0]), ' {0:10e}'.format(my_res[i][1]), '     {:2}'.format(res[i][0]), ' {0:10e}'.format(res[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All feature importance\n",
      "\n",
      "My: \n",
      " [0.09401709 0.04273504 0.00854701 0.         0.00854701 0.01709402\n",
      " 0.02564103 0.         0.         0.03418803 0.         0.00854701\n",
      " 0.00854701 0.00854701 0.         0.02564103 0.         0.03418803\n",
      " 0.00854701 0.01709402 0.         0.00854701 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00854701\n",
      " 0.01709402 0.00854701 0.01709402 0.02564103 0.00854701 0.04273504\n",
      " 0.02564103 0.00854701 0.00854701 0.00854701 0.         0.\n",
      " 0.00854701 0.01709402 0.         0.01709402 0.01709402 0.\n",
      " 0.00854701 0.02564103 0.         0.00854701 0.         0.02564103\n",
      " 0.00854701 0.02564103 0.05982906 0.01709402 0.         0.00854701\n",
      " 0.04273504 0.00854701 0.03418803 0.00854701 0.01709402 0.00854701\n",
      " 0.03418803 0.         0.02564103 0.         0.00854701 0.00854701\n",
      " 0.01709402 0.         0.         0.         0.         0.\n",
      " 0.         0.00854701 0.        ] \n",
      "\n",
      "\n",
      "Sklearn: \n",
      " [0.03920737 0.00311386 0.00518803 0.         0.00159076 0.01785454\n",
      " 0.00053716 0.         0.         0.02466296 0.         0.\n",
      " 0.         0.         0.         0.10377251 0.         0.00116918\n",
      " 0.00042278 0.         0.00047748 0.         0.00125862 0.00042471\n",
      " 0.         0.         0.         0.00109137 0.         0.00129342\n",
      " 0.         0.00136042 0.00388961 0.0218967  0.0017066  0.01651799\n",
      " 0.00167407 0.00348683 0.01184842 0.         0.01368312 0.\n",
      " 0.00210651 0.         0.         0.         0.         0.\n",
      " 0.0190387  0.01273891 0.         0.         0.00203963 0.02503208\n",
      " 0.01218722 0.01435813 0.27417043 0.00409233 0.00831132 0.00113909\n",
      " 0.023726   0.00893026 0.20738233 0.00540814 0.01216128 0.01312139\n",
      " 0.01516304 0.01128298 0.01950069 0.         0.00485922 0.01074976\n",
      " 0.00953664 0.         0.         0.         0.         0.\n",
      " 0.         0.0048354  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"All feature importance\\n\")\n",
    "print(\"My:\", \"\\n\", calc_feature_importances(my_clf), '\\n\\n')\n",
    "print(\"Sklearn:\", \"\\n\", clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "param_dist = {\n",
    "              \"min_samples_split\": range(2, 5),\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"max_features\": range(4, 10)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=20, cv=5)\n",
    "grid_search = GridSearchCV(clf, param_grid=param_dist, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=20, n_jobs=None,\n",
       "          param_distributions={'min_samples_split': range(2, 5), 'criterion': ['gini', 'entropy'], 'max_features': range(4, 10)},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'min_samples_split': range(2, 5), 'criterion': ['gini', 'entropy'], 'max_features': range(4, 10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=random_search.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=grid_search.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
